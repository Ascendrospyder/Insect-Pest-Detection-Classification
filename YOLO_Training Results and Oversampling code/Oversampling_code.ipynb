{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3b8335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef586204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict,Counter\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7577cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling the training dataset \n",
    "images_dir = r\"train/images\"\n",
    "labels_dir = r\"train/labels\"\n",
    "\n",
    "output_dir = r\"train_oversampled\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/images\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/labels\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db763411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling weights (based on how badly yolov12s perfomed on that class):\n",
      "Ants : 0.94\n",
      "Bees : 0.80\n",
      "Beetles : 0.96\n",
      "Caterpillars : 1.40\n",
      "Earthworms : 1.56\n",
      "Earwigs : 1.05\n",
      "Grasshoppers : 1.23\n",
      "Moths : 0.75\n",
      "Slugs : 1.02\n",
      "Snails : 0.81\n",
      "Wasps : 0.74\n",
      "Weevils : 0.73\n"
     ]
    }
   ],
   "source": [
    "# these are the test results obtained from the yolov12s model\n",
    "results = {\"per_class_metrics\":[\n",
    "{\"Class\": \"Ants\",\"mAP50\":0.81353,\"Box-R\": 0.71264},\n",
    "    {\"Class\":\"Bees\", \"mAP50\":0.9322, \"Box-R\": 0.86364},\n",
    "    {\"Class\":\"Beetles\", \"mAP50\": 0.80151, \"Box-R\": 0.68182},\n",
    "    {\"Class\": \"Caterpillars\",\"mAP50\": 0.56236, \"Box-R\": 0.44584},\n",
    "        {\"Class\": \"Earthworms\",  \"mAP50\": 0.50175, \"Box-R\": 0.40672},\n",
    "        {\"Class\": \"Earwigs\",\"mAP50\": 0.73984, \"Box-R\":0.61355},\n",
    "        {\"Class\": \"Grasshoppers\", \"mAP50\": 0.61363, \"Box-R\":0.55109},\n",
    "        {\"Class\":\"Moths\",\"mAP50\":  0.98688, \"Box-R\":0.93617},\n",
    "        {\"Class\":\"Slugs\",   \"mAP50\": 0.73489, \"Box-R\": 0.66667},\n",
    "        {\"Class\":\"Snails\",  \"mAP50\":0.89511,\"Box-R\":0.88},\n",
    "        {\"Class\":\"Wasps\", \"mAP50\": 0.96977,\"Box-R\":0.97872},\n",
    "        {\"Class\":\"Weevils\",\"mAP50\":0.99364, \"Box-R\":1.0}]}\n",
    "\n",
    "cls_names = [m[\"Class\"] for  m in results[\"per_class_metrics\"]]\n",
    "mAPs = [m[\"mAP50\"] for m  in results[\"per_class_metrics\"]]\n",
    "recalls = [m[\"Box-R\"] for  m in results[\"per_class_metrics\"]]\n",
    "\n",
    "\"\"\"below is the formula that I used for\n",
    "computing the oversampling weights, we give\n",
    "more importance to the mAP metric\n",
    "\n",
    "\"\"\"\n",
    "combined_score = np.array(mAPs)*0.6 + np.array(recalls)*0.4\n",
    "inv_weights =1.0 /(combined_score +1e-10)\n",
    "norm_weights = inv_weights / inv_weights.mean()  # normalizing thw weights\n",
    "\n",
    "weights = dict(zip(cls_names, norm_weights))\n",
    "print(\"Oversampling weights (based on how badly yolov12s perfomed on that class):\")\n",
    "for k, v in weights.items():\n",
    "    print(f\"{k} : {v:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed54d93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total original images: 11502\n"
     ]
    }
   ],
   "source": [
    "class_to_indices = defaultdict(list)\n",
    "label_files = sorted(os.listdir(labels_dir))\n",
    "\n",
    "for idx, lf in enumerate(label_files):\n",
    "\n",
    "    with open(os.path.join(labels_dir,lf),\"r\") as f:\n",
    "        lines = f.read().strip().splitlines()\n",
    "\n",
    "    classes_in_img = {int(line.split()[0]) for  line in lines}\n",
    "    \n",
    "    for cls in classes_in_img:\n",
    "        class_to_indices[cls].append(idx)\n",
    "\n",
    "N = len(label_files)\n",
    "print(f\"Total original images: {N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "633a7f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGES PER CLASS in the Orignal  training dataset : \n",
      "Ants : 1032 images\n",
      "Bees : 1101 images\n",
      "Beetles : 857 images\n",
      "Caterpillars : 912 images\n",
      "Earthworms : 720 images\n",
      "Earwigs : 942 images\n",
      "Grasshoppers : 1044 images\n",
      "Moths : 1059 images\n",
      "Slugs : 797 images\n",
      "Snails : 1085 images\n",
      "Wasps : 1014 images\n",
      "Weevils : 960 images\n"
     ]
    }
   ],
   "source": [
    "print(\"IMAGES PER CLASS in the Orignal  training dataset : \")\n",
    "for cls_id,name in enumerate(cls_names):\n",
    "    count =len(class_to_indices.get(cls_id,[]))\n",
    "    print(f\"{name:} : {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "818f8d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying original training images and their abels\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# the new training dataset is the orignal data + the oversampled data\n",
    "print(\"Copying original training images and their abels\")\n",
    "\n",
    "for img_file in os.listdir(images_dir):\n",
    "    shutil.copy(os.path.join(images_dir, img_file),\n",
    "        os.path.join(output_dir,\"images\", img_file))\n",
    "\n",
    "for lbl_file in os.listdir(labels_dir):\n",
    "    shutil.copy(os.path.join(labels_dir,lbl_file),\n",
    "        os.path.join(output_dir,  \"labels\",lbl_file))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9d2504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling summary:\n",
      "Original images: 11502\n",
      "Extra total images oversampled: 1077\n",
      "Caterpillars: +368\n",
      "Earthworms: +404\n",
      "Earwigs: +47\n",
      "Grasshoppers: +240\n",
      "Slugs: +18\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(output_dir, \"images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"labels\"), exist_ok=True)\n",
    "\n",
    "label_files = sorted(os.listdir(labels_dir))\n",
    "class_to_indices = defaultdict(list)\n",
    "\n",
    "for idx, txt_file in enumerate(label_files):\n",
    "    with open(os.path.join(labels_dir, txt_file)) as f:\n",
    "        lines = f.read().strip().splitlines()\n",
    "\n",
    "    classes = {int(line.split()[0]) for line in lines}\n",
    "\n",
    "    for cls in classes:\n",
    "        class_to_indices[cls].append(idx)\n",
    "\n",
    "# cls to id number mapping\n",
    "class_to_idx = {name: i for i,name in enumerate(cls_names)}\n",
    "\n",
    "\n",
    "sampled_indices = []\n",
    "max_oversampling = 2.0  # at max we double the amount of data although that wont happen in our case\n",
    "extra_counts_per_class =  {name:0 for name in cls_names}\n",
    "\n",
    "for cls_name,weight in weights.items():\n",
    "\n",
    "    if weight <=1.0:\n",
    "        continue  # only oversample weak classes\n",
    "\n",
    "    cls_idx = class_to_idx[cls_name]\n",
    "    orig_indices =class_to_indices.get(cls_idx, [])\n",
    "\n",
    "    if not orig_indices:\n",
    "        continue\n",
    "\n",
    "    multiplier =  min(weight,max_oversampling)\n",
    "    desired_count =  int(round(len(orig_indices) *multiplier))\n",
    "    extra_needed =  desired_count - len(orig_indices)\n",
    "\n",
    "    if extra_needed <= 0: # no extra need for class then skip\n",
    "        continue\n",
    "\n",
    "    sampled = random.choices(orig_indices,k=extra_needed)\n",
    "    sampled_indices.extend(sampled)\n",
    "    extra_counts_per_class[cls_name] = extra_needed\n",
    "\n",
    "print(\"Oversampling summary:\")\n",
    "print(\"Original images:\",len(label_files))\n",
    "\n",
    "print(\"Extra total images oversampled:\",len(sampled_indices))\n",
    "\n",
    "for k, v in extra_counts_per_class.items():\n",
    "    if v > 0:\n",
    "        print(f\"{k}: +{v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24cf8dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from albumentations) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from albumentations) (1.16.3)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from albumentations) (6.0.3)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from albumentations) (2.12.4)\n",
      "Requirement already satisfied: albucore==0.0.24 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from albumentations) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from albumentations) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from albucore==0.0.24->albumentations) (4.2.3)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\suraj\\onedrive\\desktop\\comp9517\\project\\.venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b33f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suraj\\OneDrive\\Desktop\\COMP9517\\Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding oversampled images with correct bboxes\n",
      "Albumentations oversampling done\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "transform = A.Compose([A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.4),\n",
    "        A.Rotate(limit = 15, p = 0.5, border_mode=0)],   # rotation without using albumentations leads to incorrect bboxes\n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'],\n",
    "        min_visibility = 0.2))\n",
    "\n",
    "print(\"Adding oversampled images with correct bboxes\")\n",
    "\n",
    "counter =0\n",
    "for idx in sampled_indices:\n",
    "\n",
    "    #loading images\n",
    "    label_file = label_files[idx]\n",
    "    img_file =label_file.replace(\".txt\",\".jpg\")\n",
    "\n",
    "    img_path =os.path.join(images_dir,img_file)\n",
    "    label_path =os.path.join(labels_dir, label_file)\n",
    "\n",
    "    if not os.path.exists(img_path):\n",
    "        img_file =img_file.replace(\".jpg\", \".png\")\n",
    "        img_path = os.path.join(images_dir, img_file)\n",
    "\n",
    "    image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "    bboxes =[]\n",
    "    cls_ids =[]\n",
    "\n",
    "    with open(label_path,\"r\") as f:\n",
    "        for line in f.read().strip().split(\"\\n\"):\n",
    "\n",
    "            parts =line.split()\n",
    "            cls =int(parts[0])\n",
    "            cx,cy, w,h = map(float,parts[1:])\n",
    "\n",
    "            bboxes.append([cx,cy,w,h])\n",
    "            cls_ids.append(cls)\n",
    "\n",
    "    # using almbumentations so that the bboxes can also be changed according to the augmentations\n",
    "    augmented = transform(image = image, bboxes=bboxes,class_labels=cls_ids)\n",
    "\n",
    "    aug_img =augmented[\"image\"]\n",
    "    aug_bboxes =augmented[\"bboxes\"]\n",
    "    aug_classes =augmented[\"class_labels\"]\n",
    "\n",
    "    # saving the augmented imgs\n",
    "    new_img_name =f\"aug_{counter}_{img_file}\"\n",
    "    Image.fromarray(aug_img).save(os.path.join(output_dir, \"images\", new_img_name))\n",
    "\n",
    "    # saving back in yolo format\n",
    "    new_label_name = f\"aug_{counter}_{label_file}\"\n",
    "    with open(os.path.join(output_dir, \"labels\", new_label_name), \"w\") as f:\n",
    "        for cls, (cx,cy,w,h) in zip(aug_classes, aug_bboxes):\n",
    "            f.write(f\"{cls} {cx :.6f} {cy :.6f} {w:.6f}{h:.6f}\\n\")\n",
    "\n",
    "    counter +=1\n",
    "\n",
    "print(\"Albumentations oversampling done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4de58e",
   "metadata": {},
   "source": [
    "### Now we zip all of the splits that is oversampled training data, testing data and validation data into one zip file so that it can uploaded to colab and the model can be trained on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
